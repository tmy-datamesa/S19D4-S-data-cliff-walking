{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â›°ï¸ UÃ§urum YÃ¼rÃ¼yÃ¼ÅŸÃ¼\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Bu meydan okumada, pekiÅŸtirmeli Ã¶ÄŸrenme modellerini eÄŸitmek iÃ§in Gymnasium ve Stable Baselines3 kullanÄ±mÄ±nÄ±n temel yÃ¶nlerini Ã¶ÄŸreneceksiniz. Bu meydan okuma, ortamlarÄ± kurma, modelleri eÄŸitme, performanslarÄ±nÄ± gÃ¶rselleÅŸtirme ve son olarak eÄŸitilmiÅŸ bir modeli bir ortamla etkileÅŸim kurmak iÃ§in kullanma konularÄ±nda bÃ¼tÃ¼ncÃ¼l bir anlayÄ±ÅŸ kazandÄ±rmanÄ±z iÃ§in tasarlanmÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "TÃ¼m import ifadelerinizi notebook'unuzun en Ã¼stÃ¼ne koyarak notebook'unuzu tekrar Ã§alÄ±ÅŸtÄ±rmayÄ± kolaylaÅŸtÄ±rmak iyi bir alÄ±ÅŸkanlÄ±ktÄ±r.\n",
    "\n",
    "Meydan okumalar boyunca import ifadeleri eklemeniz gerekecek. Hepsini aÅŸaÄŸÄ±daki hÃ¼creye ekleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym  # Oyun alanÄ±nÄ± (dÃ¼nyayÄ±) oluÅŸturmak iÃ§in\n",
    "import numpy as np       # Matematiksel hesaplamalar ve diziler iÃ§in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ¯ Meydan OkumanÄ±n Hedefleri\n",
    "\n",
    "#### ğŸ‹ï¸â€â™€ï¸ Gymnasium'a AÅŸinalÄ±k KazanÄ±n:\n",
    "Gymnasium'dan [CliffWalking ortamÄ±nÄ±](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) keÅŸfederek baÅŸlayacaÄŸÄ±z. OrtamlarÄ± yÃ¶netmek ve gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lan temel yÃ¶ntemleri anlayÄ±n. OrtamlarÄ± yÃ¼klemeyi ve sÄ±fÄ±rlamayÄ±, eylemler almayÄ± ve sonuÃ§larÄ± gÃ¶rselleÅŸtirmeyi Ã¶ÄŸrenin.\n",
    "\n",
    "#### ğŸ¤– Stable Baselines3 KullanmayÄ± Ã–ÄŸrenin:\n",
    "Bir pekiÅŸtirmeli Ã¶ÄŸrenme modelini kurmak ve eÄŸitmek iÃ§in Stable Baselines3 kÃ¼tÃ¼phanesini kullanÄ±n. Bir modeli yapÄ±landÄ±rmayÄ±, eÄŸitim parametrelerini ayarlamayÄ± ve Deep Q-Network (DQN) gibi popÃ¼ler bir algoritma kullanarak eÄŸitim sÃ¼recini baÅŸlatmayÄ± Ã¶ÄŸreneceksiniz.\n",
    "\n",
    "#### ğŸ“ˆ EÄŸitim PerformansÄ±nÄ± GÃ¶rselleÅŸtirin:\n",
    "Modelinizin eÄŸitim performansÄ±nÄ± izlemek ve Ã§izmek iÃ§in gÃ¼nlÃ¼k tutma ve TensorBoard gibi gÃ¶rselleÅŸtirme araÃ§larÄ±nÄ± uygulayÄ±n. EÄŸitim stratejilerinizin etkinliÄŸini deÄŸerlendirmek iÃ§in bÃ¶lÃ¼m baÅŸÄ±na Ã¶dÃ¼ller ve Ã¶ÄŸrenme eÄŸrileri gibi metrikleri analiz edin.\n",
    "\n",
    "#### ğŸ’¾ EÄŸitilmiÅŸ Bir Modeli YÃ¼kleyin ve DaÄŸÄ±tÄ±n:\n",
    "EÄŸitim sonrasÄ±nda, eÄŸitilmiÅŸ bir modeli kaydetmeyi ve daha sonra yÃ¼klemeyi Ã¶ÄŸrenin. Bu modeli, ajanÄ±n CliffWalking ortamÄ±yla etkileÅŸim kurduÄŸu bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±n ve Ã¶ÄŸrenilen politikalarÄ± ortamda etkili bir ÅŸekilde gezinmek iÃ§in uygulayÄ±n.\n",
    "\n",
    "#### ğŸ” DeÄŸerlendirin ve DÃ¼ÅŸÃ¼nÃ¼n:\n",
    "EÄŸitilmiÅŸ modelin ortam iÃ§indeki gerÃ§ek zamanlÄ± etkileÅŸimlerdeki performansÄ±nÄ± deÄŸerlendirin. Ã–ÄŸrenme sÃ¼reci ve ajanÄ±n davranÄ±ÅŸÄ± Ã¼zerine dÃ¼ÅŸÃ¼nÃ¼n, farklÄ± yapÄ±landÄ±rmalarÄ±n ve eÄŸitim sÃ¼relerinin sonuÃ§larÄ± nasÄ±l etkileyebileceÄŸini anlayÄ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 1: CliffWalking OrtamÄ±nÄ± KeÅŸfetmek\n",
    "\n",
    "Bu ilk gÃ¶revde, Gymnasium'dan [CliffWalking ortamÄ±nÄ±](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) kuracaksÄ±nÄ±z.\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "\n",
    "#### 0. âš™ï¸ Paketi iÃ§e aktarÄ±n:\n",
    "Notebook'unuzun en Ã¼stÃ¼ndeki hÃ¼creye gymnasium'u iÃ§e aktarÄ±n. Kodumuzda `gym` olarak kullanabildiÄŸimizden emin olun.\n",
    "\n",
    "#### 1. ğŸ—‚ï¸ OrtamÄ± YÃ¼kleyin:\n",
    "- `CliffWalking` ortamÄ±nÄ± yÃ¼klemek iÃ§in `.make()` metodunu kullanÄ±n.  \n",
    "- OrtamÄ± gÃ¶rselleÅŸtirmek iÃ§in `render_mode`'u uygun ÅŸekilde ayarlayÄ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not: `gymnasium` dokÃ¼mantasyonu size `CliffWalking-v1` kullanmanÄ±zÄ± sÃ¶ylÃ¼yor, ancak bu henÃ¼z uyumluluk nedenleriyle kullandÄ±ÄŸÄ±mÄ±z `gymnasium` sÃ¼rÃ¼m 1.0.0'da bulunmuyor. Bunun yerine `CliffWalking-v0` kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# DÃ¼nyayÄ± (environment) oluÅŸturuyoruz\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ğŸ”„ OrtamÄ± BaÅŸlatÄ±n:\n",
    "OrtamÄ± baÅŸlangÄ±Ã§ durumuna getirmek iÃ§in `.reset()` metodunu Ã§aÄŸÄ±rÄ±n â€” onunla etkileÅŸim kurmadan Ã¶nce gereklidir. `.reset()` metodu bir durum dÃ¶ndÃ¼rÃ¼r: baÅŸlangÄ±Ã§ durumunu. Bunu bir deÄŸiÅŸkende saklayÄ±n ve yazdÄ±rÄ±n.\n",
    "\n",
    "> `UserWarning: pkg_resources is deprecated ...` uyarÄ±sÄ±nÄ± gÃ¶rebilirsiniz. Bunu gÃ¶rmezden gelebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tumay/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaÅŸlangÄ±Ã§ Durumu: 36\n",
      "Ek Bilgi: {'prob': 1}\n"
     ]
    }
   ],
   "source": [
    "# AjanÄ± baÅŸlangÄ±Ã§ noktasÄ±na koy ve ilk durumu (state) al\n",
    "observation, info = env.reset()\n",
    "\n",
    "print(f\"BaÅŸlangÄ±Ã§ Durumu: {observation}\")\n",
    "print(f\"Ek Bilgi: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(36, {'prob': 1})` Ã§Ä±ktÄ±sÄ± ortamÄ±n durumu hakkÄ±nda iki bilgi parÃ§asÄ± saÄŸlar:\n",
    "\n",
    "**ğŸ”¢ Durum Ä°ndeksi (36)**: Bu sayÄ±, ajanÄ±n baÅŸlangÄ±Ã§ta ortam Ä±zgarasÄ± iÃ§indeki belirli konumunu temsil eder. Ã–rneÄŸin 'CliffWalking-v0' ortamÄ±nda, `36` indeksi ajanÄ±n bÃ¶lÃ¼mÃ¼ baÅŸlattÄ±ÄŸÄ± baÅŸlangÄ±Ã§ durumuna karÅŸÄ±lÄ±k gelir.\n",
    "\n",
    "**ğŸ² OlasÄ±lÄ±k Bilgisi ({'prob': 1})**: Bu sÃ¶zlÃ¼k, durum hakkÄ±nda, Ã¶zellikle geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ± hakkÄ±nda ek ayrÄ±ntÄ±lar gÃ¶sterir. `'prob'` anahtarÄ±nÄ±n `1` deÄŸeri, bu duruma geÃ§iÅŸin 1 olasÄ±lÄ±ÄŸÄ±yla gerÃ§ekleÅŸtiÄŸini, yani kesin olduÄŸunu gÃ¶sterir. Bu mantÄ±klÄ±dÄ±r Ã§Ã¼nkÃ¼ ajan mutlaka bu durumda baÅŸlayacaktÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ‘€ OrtamÄ± GÃ¶rselleÅŸtirin:\n",
    "Herhangi bir eylem almadan Ã¶nce ortamÄ± gÃ¶rÃ¼ntÃ¼lemek ve dÃ¼zenini anlamak iÃ§in `.render()` metodunu kullanÄ±n.\n",
    "\n",
    "Kurulumunuza baÄŸlÄ± olarak, yeni durum Ã¶nceki adÄ±mdan sonra zaten iÅŸlenmiÅŸ olabileceÄŸi iÃ§in bu adÄ±ma ihtiyacÄ±nÄ±z olmayabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ğŸ§¹ OrtamÄ± KapatÄ±n:\n",
    "Ä°ÅŸiniz bittiÄŸinde kaynaklarÄ± serbest bÄ±rakmak iÃ§in `.close()` Ã§aÄŸÄ±rarak ortamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatÄ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary markdown='span'>âš ï¸ <strong>macOS iÃ§in kapatma hakkÄ±nda not</strong></summary>\n",
    "  \n",
    "  `.close()` ortam penceresini kapatmayacak. Bu sorun deÄŸil, **aÃ§Ä±k bÄ±rakabilirsiniz**. Pencere sonraki adÄ±mlarda tekrar kullanÄ±lacak.\n",
    "\n",
    "  Meydan okumanÄ±n sonunda ve **sadece sonunda**, ortam penceresini kapatmak iÃ§in onu kendiniz zorla sonlandÄ±rmanÄ±z gerekecek:\n",
    "  1. Pencereyi bulun ve kapatmak iÃ§in kÄ±rmÄ±zÄ± dÃ¼ÄŸmeye tÄ±klayÄ±n. (Bu baÅŸarÄ±sÄ±z olacak.)\n",
    "  1. EkranÄ±nÄ±zÄ±n sol Ã¼st kÃ¶ÅŸesinde Apple sembolÃ¼ne tÄ±klayÄ±n.\n",
    "  1. `Force Quit`'i (Zorla Ã‡Ä±kÄ±ÅŸ) seÃ§in.\n",
    "  1. Ä°ÅŸlem listesinde `python (Not Responding)` iÅŸlemini bulun. Onu seÃ§in.\n",
    "  1. `Force Quit` dÃ¼ÄŸmesine tÄ±klayÄ±n.\n",
    "\n",
    "  Bu Ã§ekirdeÄŸinizi sonlandÄ±racaÄŸÄ±ndan, **bunu sadece meydan okumanÄ±n sonunda yapÄ±n**.\n",
    "\n",
    "  Bu durum Jupyter Notebook iÃ§inde Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±z iÃ§in oluyor. Kodunuzu bir `.py` dosyasÄ±na taÅŸÄ±rsanÄ±z, bu davranÄ±ÅŸÄ± gÃ¶rmezsiniz.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 2: Ortamla EtkileÅŸim Kurmak\n",
    "\n",
    "Åimdi ortamÄ± tekrar yÃ¼kleyelim, rastgele bir adÄ±m atalÄ±m ve sonucu gÃ¶sterelim. Bu, ortamlarla etkileÅŸim kurmak iÃ§in temel metotlarÄ± tanÄ±tacak â€” ajanlarÄ±n nasÄ±l hareket ettiÄŸini ve geri bildirim aldÄ±ÄŸÄ±nÄ± anlamanÄ±za yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ğŸ—‚ï¸ OrtamÄ± YÃ¼kleyin\n",
    "- OrtamÄ± yÃ¼kleyin.\n",
    "- BaÅŸlangÄ±Ã§ durumuna sÄ±fÄ±rlayÄ±n.\n",
    "- BaÅŸlangÄ±Ã§ durumunu yazdÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaÅŸlangÄ±Ã§ Durumu: 36\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# OrtamÄ± tekrar yÃ¼klÃ¼yoruz\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ durumuna getiriyoruz\n",
    "observation, info = env.reset()\n",
    "print(f\"BaÅŸlangÄ±Ã§ Durumu: {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ğŸ² Bir Eylem Ã–rnekleyin:\n",
    "- OrtamÄ±n eylem alanÄ±ndan rastgele bir eylem seÃ§mek iÃ§in `.action_space.sample()` kullanÄ±n â€” keÅŸfi simÃ¼le eder.\n",
    "- Bunu bir deÄŸiÅŸkende kaydedin.\n",
    "- Eylemin tÃ¼rÃ¼nÃ¼ ve deÄŸerini inceleyin.\n",
    "- HÃ¼creyi birkaÃ§ kez Ã§alÄ±ÅŸtÄ±rÄ±n. Hangi deÄŸerleri gÃ¶rÃ¼yorsunuz? Bunlar neyi temsil ediyor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeÃ§ilen Rastgele Eylem: 3\n"
     ]
    }
   ],
   "source": [
    "# Rastgele bir eylem seÃ§\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# SeÃ§ilen eylemi kontrol et (0: YukarÄ±, 1: SaÄŸ, 2: AÅŸaÄŸÄ±, 3: Sol)\n",
    "print(f\"SeÃ§ilen Rastgele Eylem: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ğŸ¦¶ Eylemi GerÃ§ekleÅŸtirin:\n",
    "- `.step()` kullanarak eylemi uygulayÄ±n.  \n",
    "- Bu ÅŸunlarÄ± dÃ¶ndÃ¼rÃ¼r:  \n",
    "  - Yeni durum  \n",
    "  - Ã–dÃ¼l  \n",
    "  - Bir `done` bayraÄŸÄ± (bÃ¶lÃ¼m bitmiÅŸ mi deÄŸil mi)  \n",
    "  - Ek bilgi (varsa)\n",
    "- \"tuple unpacking\" kullanarak bu dÃ¶nen deÄŸerlerin her birini bir deÄŸiÅŸkende kaydedin. OnlarÄ± yazdÄ±rÄ±n. Yeni durum, Ã¶dÃ¼l ve done dÃ¶nÃ¼ÅŸ deÄŸerlerini anlÄ±yor musunuz?\n",
    "- Bu **sÄ±fÄ±rla > eylem > adÄ±m** dizisini birkaÃ§ kez Ã§alÄ±ÅŸtÄ±rÄ±n ve farklÄ± sonuÃ§larÄ± kontrol edin.\n",
    "\n",
    "<details>\n",
    "  <summary markdown='span'>\n",
    "  ğŸ’¡ Tuple unpacking?\n",
    "  </summary>\n",
    "\n",
    "  Bir fonksiyon tuple dÃ¶ndÃ¼rÃ¼rse, tuple'Ä±n farklÄ± elemanlarÄ±nÄ± hemen farklÄ± deÄŸiÅŸkenlere kaydedebilirsiniz.\n",
    "\n",
    "  Ã–rnek:\n",
    "\n",
    "  Åu fonksiyonunuz olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼n:\n",
    "\n",
    "  ```python\n",
    "  def surface_and_circumference(a, b):\n",
    "    \"\"\"UzunluÄŸu `a` ve geniÅŸliÄŸi `b` olan dikdÃ¶rtgenin \n",
    "    alanÄ±nÄ± ve Ã§evresini dÃ¶ndÃ¼rÃ¼r.\"\"\"\n",
    "    return a*b, 2*a + 2*b\n",
    "  ```\n",
    "\n",
    "  Bunun yerine:\n",
    "\n",
    "  ```python\n",
    "  result = surface_and_circumference(4, 2)\n",
    "  surface = result[0]\n",
    "  circumference = result[1]\n",
    "  ```\n",
    "\n",
    "  Hemen ÅŸunu yapabilirsiniz:\n",
    "\n",
    "  ```python\n",
    "  surface, circumference = surface_and_circumference(4, 2)\n",
    "  ```\n",
    "\n",
    "  Kodunuzun geri kalanÄ±nda sadece surface kullanacaksanÄ±z, diÄŸer dÃ¶nÃ¼ÅŸ deÄŸerleri iÃ§in `_` kullanmak yaygÄ±n bir alÄ±ÅŸkanlÄ±ktÄ±r. Bu, diÄŸer programcÄ±lara kalan deÄŸerleri attÄ±ÄŸÄ±nÄ±zÄ±n bir iÅŸaretidir.\n",
    "\n",
    "  Ã–rnek:\n",
    "  \n",
    "  ```python\n",
    "  surface, _ = surface_and_circumference(4, 2)\n",
    "  # Buradan sonra sadece surface'a ihtiyaÃ§ duyan kod gelir\n",
    "  ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Yeni Durum (Kutu No): 36\n",
      "ğŸ’¸ AlÄ±nan Ã–dÃ¼l: -1\n",
      "ğŸš¦ BÃ¶lÃ¼m Bitti mi?: False\n"
     ]
    }
   ],
   "source": [
    "# AdÄ±mÄ± atÄ±yoruz\n",
    "new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"ğŸ“ Yeni Durum (Kutu No): {new_state}\")\n",
    "print(f\"ğŸ’¸ AlÄ±nan Ã–dÃ¼l: {reward}\")\n",
    "print(f\"ğŸš¦ BÃ¶lÃ¼m Bitti mi?: {terminated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ“ Yeni Durum**: Bu sayÄ±, belirtilen eylemi gerÃ§ekleÅŸtirdikten sonra ajanÄ±n durumunu temsil eder. `CliffWalking` ortamÄ±nda durum, ajanÄ±n hareket ettiÄŸi ortam Ä±zgarasÄ±ndaki belirli bir konuma karÅŸÄ±lÄ±k gelir.\n",
    "\n",
    "**ğŸ’¸ Ã–dÃ¼l**: Ã–dÃ¼l deÄŸeri, ajanÄ±n eyleminin sonucu olarak ortam tarafÄ±ndan verilen anlÄ±k geri bildirimi gÃ¶sterir. BirÃ§ok Ä±zgara tabanlÄ± ortamda, bunun gibi negatif bir Ã¶dÃ¼l genellikle bir cezayÄ± ifade eder ve alÄ±nan eylemin optimal olmayabileceÄŸini veya diÄŸer stratejileri teÅŸvik etmek iÃ§in cezalandÄ±rÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "**ğŸš¦ Done**: Boolean deÄŸer bÃ¶lÃ¼mÃ¼n bitip bitmediÄŸini gÃ¶sterir. Bu durumda, `False` bÃ¶lÃ¼mÃ¼n hala devam ettiÄŸi ve ajanÄ±n bÃ¶lÃ¼mÃ¼ sona erdirecek terminal bir duruma (hedef veya Ã§ukur gibi) ulaÅŸmadÄ±ÄŸÄ± anlamÄ±na gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ‘ï¸ Sonucu GÃ¶rselleÅŸtirin:\n",
    "- Eylem sonrasÄ±nda ortamÄ± gÃ¶rselleÅŸtirmek iÃ§in tekrar `.render()` Ã§aÄŸÄ±rÄ±n â€” ve durumun nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶rÃ¼n.\n",
    "- Render penceresi gÃ¶rmÃ¼yorsanÄ±z, muhtemelen diÄŸer pencerelerinizin arkasÄ±nda gizli veya baÅŸka bir masaÃ¼stÃ¼nde gÃ¶steriliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eski Durum: 36\n",
      "Yeni Durum (YUKARI gidince): 24\n",
      "Ã–dÃ¼l: -1\n"
     ]
    }
   ],
   "source": [
    "# OrtamÄ± sÄ±fÄ±rla (temiz bir baÅŸlangÄ±Ã§ iÃ§in)\n",
    "state, _ = env.reset()\n",
    "\n",
    "# BilinÃ§li eylem: 0 (YUKARI)\n",
    "action = 0 \n",
    "\n",
    "# AdÄ±mÄ± at\n",
    "new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(f\"Eski Durum: 36\")\n",
    "print(f\"Yeni Durum (YUKARI gidince): {new_state}\")\n",
    "print(f\"Ã–dÃ¼l: {reward}\")\n",
    "\n",
    "# GÃ¶rselleÅŸtir\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ğŸ§¹ OrtamÄ± KapatÄ±n:\n",
    "Ä°ÅŸiniz bittiÄŸinde kaynaklarÄ± serbest bÄ±rakmak iÃ§in `.close()` Ã§aÄŸÄ±rarak ortamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 3: Ortamla YÃ¶nlendirilmiÅŸ EtkileÅŸim\n",
    "\n",
    "Bu sefer rastgele bir eylem seÃ§mek yerine kasÄ±tlÄ± bir adÄ±m atacaksÄ±nÄ±z â€” **YUKARI** hareket edeceÄŸiz. Bu, pekiÅŸtirmeli Ã¶ÄŸrenmede amaÃ§lÄ± karar verme fikrini pekiÅŸtirir.\n",
    "\n",
    "#### 1. ğŸš€ BaÅŸlatÄ±n ve Ä°lk Durumu YazdÄ±rÄ±n:\n",
    "- OrtamÄ± yÃ¼kleyin ve baÅŸlangÄ±Ã§ durumunu almak iÃ§in `.reset()` Ã§aÄŸÄ±rÄ±n.  \n",
    "- Herhangi bir eylem almadan Ã¶nce baÅŸlangÄ±Ã§ durumunu gÃ¶rÃ¼ntÃ¼lemek iÃ§in `print()` kullanÄ±n.\n",
    "- Åu Ã§Ä±ktÄ±yÄ± almalÄ±sÄ±nÄ±z `(36, {'prob': 1})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, {'prob': 1})\n"
     ]
    }
   ],
   "source": [
    "# Load the environment again, and initialize it\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "\n",
    "# Reset the environment to the initial state\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Print the initial state\n",
    "print(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. â¬†ï¸ 'YUKARI' Eylemini Belirleyin ve GerÃ§ekleÅŸtirin\n",
    "\n",
    "- `.action_space`'i ve dokÃ¼mantasyonu kontrol ederek **'YUKARI'** eylemi iÃ§in indeksi bulun.  \n",
    "- Bu eylemi gerÃ§ekleÅŸtirmek iÃ§in `.step(action_index)` kullanÄ±n.  \n",
    "- ÅunlarÄ± gÃ¶rmek iÃ§in sonucu yazdÄ±rÄ±n:\n",
    "  - Yeni durum  \n",
    "  - AlÄ±nan Ã¶dÃ¼l  \n",
    "  - BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸi (`done` bayraÄŸÄ±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeni Durum: 24\n",
      "AlÄ±nan Ã–dÃ¼l: -1\n",
      "BÃ¶lÃ¼m Bitti mi?: False\n"
     ]
    }
   ],
   "source": [
    "# Take a step to move 'UP' (Index 0)\n",
    "action = 0\n",
    "\n",
    "# Take the action\n",
    "new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# Print the new state, reward, and done status\n",
    "print(f\"Yeni Durum: {new_state}\")\n",
    "print(f\"AlÄ±nan Ã–dÃ¼l: {reward}\")\n",
    "print(f\"BÃ¶lÃ¼m Bitti mi?: {terminated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ–¼ï¸ OrtamÄ± GÃ¶rselleÅŸtirin ve KapatÄ±n\n",
    "\n",
    "- Eylem sonrasÄ±nda ortamÄ± gÃ¶rselleÅŸtirmek iÃ§in `.render()` kullanÄ±n â€” durumun nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶rÃ¼n.\n",
    "- ArdÄ±ndan ortamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatmak ve sistem kaynaklarÄ±nÄ± serbest bÄ±rakmak iÃ§in `.close()` Ã§aÄŸÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "# Render the environment after taking a step\n",
    "env.render()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 4: Bir RL AlgoritmasÄ± EÄŸitmek\n",
    "\n",
    "Ä°lk olarak, `CliffWalking-v0` ortamÄ±nÄ± Stable Baselines3 modelleriyle eÄŸitim iÃ§in hazÄ±rlayacaksÄ±nÄ±z. Bu, RL algoritmalarÄ±yla uyumluluÄŸu saÄŸlamak iÃ§in uygun kurulum ve sarmalama iÃ§erir.\n",
    "\n",
    "#### ğŸ§± 1. OrtamÄ± YÃ¼kleyin:\n",
    "- `CliffWalking-v0` ortamÄ±nÄ± oluÅŸturmak iÃ§in `gym.make()` kullanÄ±n.  \n",
    "- `render_mode='human'` ayarlayÄ±n â†’ EtkileÅŸim sÄ±rasÄ±nda gÃ¶rsel geri bildirim saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# OrtamÄ± 'human' modunda oluÅŸturuyoruz\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§µ 2. OrtamÄ± SarÄ±n\n",
    "\n",
    "OrtamÄ±nÄ±zÄ± [`DummyVecEnv`](https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html#dummyvecenv) ile sarmak kÃ¼Ã§Ã¼k ama kritik bir adÄ±mdÄ±r â€” RL kurulumunuzun Stable Baselines3 ile sorunsuz Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.\n",
    "\n",
    "Devam edin ve ortamÄ± Stable Baselines3'ten [`DummyVecEnv`](https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html#dummyvecenv) kullanarak sarÄ±n.  ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# OrtamÄ± SB3 ile uyumlu hale getirmek iÃ§in sarmalÄ±yoruz\n",
    "env = DummyVecEnv([lambda: env]) # DoÄŸru ortam iÅŸlemeyi saÄŸlamak iÃ§in lambda fonksiyonu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§µ `DummyVecEnv` HakkÄ±nda Daha Fazla\n",
    "\n",
    "`DummyVecEnv`, Gym ortamlarÄ±nÄ± vektÃ¶rleÅŸtirerek RL modelleriyle uyumlu hale getiren Stable Baselines3'ten bir sarmalayÄ±cÄ±dÄ±r.\n",
    "\n",
    "#### âš™ï¸ Ne Yapar?\n",
    "\n",
    "- **ğŸ“¦ API'yi StandartlaÅŸtÄ±rÄ±r**  \n",
    "  OrtamÄ±n Stable Baselines3 algoritmalarÄ± iÃ§in beklenen formatla eÅŸleÅŸmesini saÄŸlar.\n",
    "\n",
    "- **ğŸ“Š Toplu Ä°ÅŸlemeyi EtkinleÅŸtirir**  \n",
    "  Tek bir ortamla bile etkileÅŸimler bir toplu gibi iÅŸlenir â€” daha sonra `SubprocVecEnv` gibi araÃ§larla Ã¶lÃ§ekleme iÃ§in gereklidir.\n",
    "\n",
    "- **ğŸ”— UyumluluÄŸu SaÄŸlar**  \n",
    "  `reset()` ve `step()`'i eÄŸitim dÃ¶ngÃ¼leri iÃ§inde doÄŸru Ã§alÄ±ÅŸacak ÅŸekilde sarar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SB3'den Bir DQN Modeli BaÅŸlatÄ±n\n",
    "\n",
    "ArtÄ±k ortamÄ±nÄ±z hazÄ±r olduÄŸuna gÃ¶re, modeli baÅŸlatmanÄ±z gerekir. Bu durumda, Stable Baselines3 kullanarak bir Deep Q-Network (DQN) yapÄ±landÄ±racaksÄ±nÄ±z. DQN, Q-deÄŸerlerini tahmin etmek iÃ§in bir sinir aÄŸÄ± kullanÄ±r â€” belirli bir durumda her eylem iÃ§in beklenen Ã¶dÃ¼lÃ¼ tahmin eder.\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "- `stable_baselines3`'den `DQN`'i iÃ§e aktarÄ±n. Bunu notebook'un en Ã¼stÃ¼ndeki hÃ¼creye ekleyin.\n",
    "- AÅŸaÄŸÄ±daki parametrelerle DQN modelinin bir Ã¶rneÄŸini baÅŸlatÄ±n:\n",
    "    - `'MlpPolicy'` kullanÄ±n â€” gÃ¶zlemleri eylemlere eÅŸleyen temel bir sinir aÄŸÄ±.  \n",
    "    - `env`'i ortamÄ±nÄ±za ve ayrÄ±ntÄ±lÄ± eÄŸitim gÃ¼nlÃ¼kleri iÃ§in `verbose=1`'e ayarlayÄ±n.\n",
    "    - EÄŸitim metriklerini izlemek iÃ§in `tensorboard_log` parametresi ekleyin. Bunu daha sonra TensorBoard ile eÄŸitimi takip etmek iÃ§in kullanacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# DQN modelini baÅŸlatÄ±yoruz\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./dqn_cliff_tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ğŸ‹ï¸â€â™‚ï¸ DQN Modelini EÄŸitin ve Kaydedin\n",
    "\n",
    "DQN modelinizi eÄŸitme ve gelecekte kullanmak Ã¼zere kaydetme zamanÄ± ğŸ¥³\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "\n",
    "#### â±ï¸ EÄŸitim AdÄ±m SayÄ±sÄ±nÄ± AyarlayÄ±n\n",
    "- Modeli ne kadar sÃ¼re eÄŸiteceÄŸinizi (ortamla etkileÅŸim sayÄ±sÄ± cinsinden) tanÄ±mlayÄ±n.  \n",
    "- Åimdilik ÅŸunu kullanÄ±n: `total_timesteps = 1000`\n",
    "\n",
    "#### ğŸ§  Modeli EÄŸitin\n",
    "- EÄŸitimi baÅŸlatmak iÃ§in DQN modelinizde `.learn()` Ã§aÄŸÄ±rÄ±n.  \n",
    "- Model, geri bildirimlere dayanarak zaman iÃ§inde politikasÄ±nÄ± geliÅŸtirecek.\n",
    "\n",
    "#### ğŸ’¾ EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "- Modeli diske kaydetmek iÃ§in `.save()` kullanÄ±n.  \n",
    "- DoÄŸru kaydedildiÄŸini doÄŸrulamak iÃ§in dosya yolunu yazdÄ±rÄ±n.\n",
    "\n",
    "ğŸ¥ EÄŸitim sÄ±rasÄ±nda iÅŸlenmiÅŸ ortamdaki adÄ±mlarÄ± takip edebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0c5bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6c88931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_cliff_tensorboard/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7ee6d3cfb240f3ac496ae160d2077f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7435f283db50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'dqn_cliffwalking_v1' adÄ±yla baÅŸarÄ±yla kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"dqn_cliffwalking_v1\")\n",
    "print(\"Model 'dqn_cliffwalking_v1' adÄ±yla baÅŸarÄ±yla kaydedildi!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tebrikler, ilk RL modelinizi eÄŸittiniz!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BÃ¶lÃ¼m 5: GÃ¶rselleÅŸtirme Olmadan Verimli EÄŸitim\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, ortamÄ± hazÄ±rlama, modeli yÃ¼kleme ve eÄŸitme sÃ¼recinin tamamÄ±ndan geÃ§eceksiniz, ancak gÃ¶rsel iÅŸleme olmadan. EÄŸitim aÅŸamasÄ±nda gÃ¶rselleÅŸtirmeyi devre dÄ±ÅŸÄ± bÄ±rakmak, hesaplama yÃ¼kÃ¼nÃ¼ azalttÄ±ÄŸÄ± iÃ§in eÄŸitim sÃ¼recini Ã¶nemli Ã¶lÃ§Ã¼de hÄ±zlandÄ±rabilir.\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "\n",
    "#### ğŸ§± 1. OrtamÄ± YÃ¼kleyin ve HazÄ±rlayÄ±n\n",
    "- GÃ¶rsel Ã§Ä±ktÄ±yÄ± devre dÄ±ÅŸÄ± bÄ±rakmak iÃ§in `render_mode=None` kullanÄ±n.\n",
    "\n",
    "#### âš™ï¸ 2. DQN Modelini YapÄ±landÄ±rÄ±n ve BaÅŸlatÄ±n\n",
    "- Daha Ã¶nce olduÄŸu gibi `'MlpPolicy'` ve `DummyVecEnv` kullanÄ±n.  \n",
    "- GÃ¼nlÃ¼k Ã§Ä±ktÄ±sÄ± istiyorsanÄ±z `verbose=1`'i koruyun.\n",
    "- TensorBoard ile eÄŸitimi takip edebilmemiz iÃ§in bir TensorBoard gÃ¼nlÃ¼kleme konumu `./dqn_tensorboard` ekleyeceÄŸiz.\n",
    "\n",
    "#### â±ï¸ 3. EÄŸitim Parametrelerini AyarlayÄ±n ve EÄŸitin\n",
    "- Daha iyi Ã¶ÄŸrenme iÃ§in adÄ±m sayÄ±sÄ±nÄ± artÄ±rÄ±n (Ã¶rn. `total_timesteps = 100_000`).  \n",
    "- EÄŸitimi baÅŸlatmak iÃ§in `.learn()` Ã§aÄŸÄ±rÄ±n.\n",
    "\n",
    "#### ğŸ’¾ 4. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "- Modelinizi saklamak iÃ§in `.save(\"dqn_cliffwalking_fast\")` kullanÄ±n.\n",
    "\n",
    "Bu adÄ±mlarÄ± izleyerek, ortamÄ± gÃ¶rselleÅŸtirmenin ek hesaplama yÃ¼kÃ¼ olmadan verimli bir ÅŸekilde bir DQN modeli eÄŸitir ve kaydedersiniz. Bu yaklaÅŸÄ±m Ã¶zellikle karmaÅŸÄ±k modeller eÄŸitirken veya sÄ±nÄ±rlÄ± hesaplama kaynaklarÄ± kullanÄ±rken faydalÄ±dÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b48427d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeb6687a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./dqn_cliff_tensorboard/DQN_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366915e2bb764ed4b91cac8c72d3b731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7435e2245cd0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 1. AdÄ±m: OrtamÄ± gÃ¶rselleÅŸtirme olmadan (hÄ±zlÄ± eÄŸitim iÃ§in) oluÅŸturun\n",
    "env = gym.make('CliffWalking-v0', render_mode=None)\n",
    "\n",
    "# 2. AdÄ±m: OrtamÄ± SB3 ile uyumlu hale getirmek iÃ§in sarmalayÄ±n\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# 3. AdÄ±m: Modeli bu yeni 'env' ile baÅŸtan yapÄ±landÄ±rÄ±n\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./dqn_cliff_tensorboard\")\n",
    "\n",
    "# 4. AdÄ±m: Åimdi eÄŸitimi baÅŸlatÄ±n\n",
    "total_timesteps = 100_000\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bf5904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'dqn_cliffwalking_fast' adÄ±yla baÅŸarÄ±yla kaydedildi! âœ…\n"
     ]
    }
   ],
   "source": [
    "# EÄŸitilmiÅŸ uzman modeli kaydediyoruz\n",
    "model.save(\"dqn_cliffwalking_fast\")\n",
    "print(\"Model 'dqn_cliffwalking_fast' adÄ±yla baÅŸarÄ±yla kaydedildi! âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9078601c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "You must set the environment before calling _setup_learn()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# EÄŸitimi baÅŸlatÄ±yoruz (progress_bar=True ile ne kadar kaldÄ±ÄŸÄ±nÄ± gÃ¶rebilirsin)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m [cite: \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/dqn/dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[1;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:314\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOffPolicyAlgorithm,\n\u001b[1;32m    307\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    313\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOffPolicyAlgorithm:\n\u001b[0;32m--> 314\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set the environment before calling learn()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:288\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_learn\u001b[0;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[1;32m    285\u001b[0m     pos \u001b[38;5;241m=\u001b[39m (replay_buffer\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m replay_buffer\u001b[38;5;241m.\u001b[39mbuffer_size\n\u001b[1;32m    286\u001b[0m     replay_buffer\u001b[38;5;241m.\u001b[39mdones[pos] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must set the environment before calling _setup_learn()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Vectorize action noise if needed\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_noise, VectorizedActionNoise)\n\u001b[1;32m    294\u001b[0m ):\n",
      "\u001b[0;31mAssertionError\u001b[0m: You must set the environment before calling _setup_learn()"
     ]
    }
   ],
   "source": [
    "# EÄŸitimi baÅŸlatÄ±yoruz (progress_bar=True ile ne kadar kaldÄ±ÄŸÄ±nÄ± gÃ¶rebilirsin)\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True) [cite: 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# GÃ¶rselleÅŸtirme olmadan (daha hÄ±zlÄ±) ortamÄ± oluÅŸturuyoruz \n",
    "env = gym.make('CliffWalking-v0', render_mode=None)\n",
    "\n",
    "# SB3 uyumluluÄŸu iÃ§in sarmalÄ±yoruz \n",
    "env = DummyVecEnv([lambda: env])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“Š EÄŸitim GÃ¼nlÃ¼k Metriklerini Anlamak\n",
    "\n",
    "Bir pekiÅŸtirmeli Ã¶ÄŸrenme modelini eÄŸitirken, Ã§eÅŸitli metrikler Ã¶ÄŸrenme ilerlemesini ve performansÄ±nÄ± izlemeye yardÄ±mcÄ± olur.\n",
    "\n",
    "#### ğŸ² Rollout Metrikleri\n",
    "\n",
    "- **exploration_rate** â†’ Rastgele bir eylem alma olasÄ±lÄ±ÄŸÄ±.  \n",
    "  - YÃ¼ksek = daha fazla keÅŸif  \n",
    "  - DÃ¼ÅŸÃ¼k = daha fazla istismar  \n",
    "\n",
    "#### â±ï¸ Zaman ile Ä°lgili Metrikler\n",
    "\n",
    "- **episodes** â†’ Tamamlanan bÃ¶lÃ¼m sayÄ±sÄ±.  \n",
    "- **fps** â†’ Saniye baÅŸÄ±na kare (eÄŸitimin ne kadar hÄ±zlÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±).  \n",
    "- **time_elapsed** â†’ EÄŸitim baÅŸladÄ±ÄŸÄ±ndan beri geÃ§en toplam sÃ¼re (saniye).  \n",
    "- **total_timesteps** â†’ Ortamda atÄ±lan toplam adÄ±m sayÄ±sÄ±.\n",
    "\n",
    "#### ğŸ§  EÄŸitim Metrikleri\n",
    "\n",
    "- **learning_rate** â†’ Model aÄŸÄ±rlÄ±klarÄ±na yapÄ±lan gÃ¼ncellemelerin boyutu.  \n",
    "  - DÃ¼ÅŸÃ¼k = daha yavaÅŸ ama daha kararlÄ± Ã¶ÄŸrenme  \n",
    "- **loss** â†’ Modelin tahmin hatasÄ±.  \n",
    "  - Azalan kayÄ±p = Ã¶ÄŸrenme Ã§alÄ±ÅŸÄ±yor  \n",
    "- **n_updates** â†’ Modelin aÄŸÄ±rlÄ±klarÄ±nÄ± kaÃ§ kez gÃ¼ncellediÄŸi.\n",
    "\n",
    "#### ğŸ” NasÄ±l YorumlanÄ±r\n",
    "\n",
    "- **â¬‡ï¸ exploration_rate** â†’ Ajan keÅŸiften Ã¶ÄŸrenilmiÅŸ davranÄ±ÅŸa geÃ§iyor.  \n",
    "- **âš¡ YÃ¼ksek fps** â†’ EÄŸitim verimli Ã§alÄ±ÅŸÄ±yor.  \n",
    "- **ğŸ“‰ Azalan kayÄ±p** â†’ Model geliÅŸiyor ve daha az hata yapÄ±yor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ–¥ï¸ TensorBoard'u BaÅŸlatÄ±n\n",
    "\n",
    "TensorBoard eÄŸitimin nasÄ±l gittiÄŸini takip etmenizi saÄŸlar. EtkileÅŸimli pano Ã¶ÄŸrenme eÄŸrilerini, Ã¶dÃ¼l trendlerini ve daha fazlasÄ±nÄ± gÃ¶sterir. \n",
    "\n",
    "TensorBoard kullanmak iÃ§in:\n",
    "\n",
    "1. Bir terminal penceresi aÃ§Ä±n.\n",
    "1. Meydan okuma klasÃ¶rÃ¼nde olduÄŸunuzdan emin olun!\n",
    "1. Bu komutu Ã§alÄ±ÅŸtÄ±rÄ±n (eÄŸitim sÄ±rasÄ±nda ayarladÄ±ÄŸÄ±nÄ±z yol ile deÄŸiÅŸtirmeniz gerekebilir):\n",
    "   ```bash\n",
    "   tensorboard --logdir=./dqn_cliff_tensorboard/\n",
    "   ```\n",
    "1. Terminalde gÃ¶sterilecek baÄŸlantÄ±yÄ± takip edin (muhtemelen `localhost:6006`).\n",
    "\n",
    "Modeliniz hala eÄŸitilirken TensorBoard'u baÅŸlatabilirsiniz: amacÄ± eÄŸitimin nasÄ±l gittiÄŸini takip etmektir. EÄŸitimin baÅŸlangÄ±cÄ±nda `No dashboards are active for the current data set.` uyarÄ±sÄ± alabilirsiniz. **Biraz sabÄ±rlÄ± olun: ilk bÃ¶lÃ¼m bitene kadar hiÃ§bir ÅŸey gÃ¶rmeyeceksiniz.**\n",
    "\n",
    "TensorBoard'u notebook'unuzun iÃ§inde aÃ§mak da mÃ¼mkÃ¼n:\n",
    "\n",
    "```bash\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./dqn_cliff_tensorboard/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ BÃ¶lÃ¼m 6: EÄŸitilmiÅŸ Bir Model KullanÄ±n\n",
    "\n",
    "SÄ±fÄ±rdan tekrar eÄŸitmek yerine, ajanÄ±nÄ±zÄ±n nasÄ±l davrandÄ±ÄŸÄ±nÄ± hÄ±zlÄ±ca gÃ¶zlemlemek iÃ§in az Ã¶nce eÄŸittiÄŸiniz modeli yÃ¼kleyebilirsiniz.  \n",
    "Alternatif olarak, mevcut olduÄŸu takdirde daha gÃ¼Ã§lÃ¼ Ã¶nceden eÄŸitilmiÅŸ bir model yÃ¼kleyebilirsiniz.\n",
    "\n",
    "#### ğŸ“¥ 1. Ã–nceden EÄŸitilmiÅŸ Modeli YÃ¼kleyin\n",
    "\n",
    "- Modeli diskten yÃ¼klemek iÃ§in `.load(\"Ã¶nceden_eÄŸitilmiÅŸ_modelinizin_yolu\")` kullanÄ±n.  \n",
    "- Yolu kaydettiÄŸiniz modelin gerÃ§ek konumuyla deÄŸiÅŸtirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# Az Ã¶nce kaydettiÄŸin modeli yÃ¼kle\n",
    "model = DQN.load(\"dqn_cliffwalking_fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "032b6645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.gitignore', 'dqn_cliffwalking_fast.zip', '.git', 'dqn_cliff_tensorboard', 'dqn_cliffwalking_v1.zip', 'README.md', 'cliff-walking.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a32078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli saklamak iÃ§in .save() kullanÄ±n\n",
    "model.save(\"dqn_cliffwalking_fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§± 2. OrtamÄ± HazÄ±rlayÄ±n\n",
    "\n",
    "EÄŸitilmiÅŸ modeli Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce yeni bir bÃ¶lÃ¼m baÅŸlatmak iÃ§in ortamÄ± yÃ¼kleyin ve sÄ±fÄ±rlayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# OrtamÄ± gÃ¶rsel modda oluÅŸtur ve sar\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# Yeni bir bÃ¶lÃ¼me baÅŸla ve baÅŸlangÄ±Ã§ durumunu al\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ§  En Ä°yi Sonraki Eylemi Tahmin Edin\n",
    "\n",
    "Mevcut gÃ¶zleme dayalÄ± olarak sonraki eylemi seÃ§mek iÃ§in modelin `.predict()` metodunu kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Modelden en iyi aksiyonu tahmin etmesini iste\n",
    "action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "# Tahmin edilen bu adÄ±mÄ± at\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# Sonucu gÃ¶r\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ğŸš¶â€â™‚ï¸ AdÄ±mÄ± AtÄ±n\n",
    "\n",
    "- Tahmin edilen eylemi uygulamak iÃ§in `.step(action)` kullanÄ±n.  \n",
    "- Bu ÅŸunlarÄ± dÃ¶ndÃ¼rÃ¼r:\n",
    "  - Yeni durum  \n",
    "  - Ã–dÃ¼l  \n",
    "  - Bir `done` bayraÄŸÄ± (bÃ¶lÃ¼m bitmiÅŸ ise)  \n",
    "  - Ek bilgi (varsa)\n",
    "\n",
    "AjanÄ±nÄ±z garip hareketler yaparsa veya sÄ±kÄ±ÅŸÄ±rsa, paniklemeyÄ±n: muhtemelen yeterince uzun eÄŸitim gÃ¶rmemiÅŸtir. Daha uzun eÄŸitmeyi denemeden Ã¶nce meydan okumanÄ±n sonuna kadar gidin ğŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tahmin edilen eylemi uygula ve geri bildirimleri al\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# SonuÃ§larÄ± yazdÄ±rarak kontrol edelim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“ Yeni Durum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "# Tahmin edilen eylemi uygula ve geri bildirimleri al\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±rarak kontrol edelim\n",
    "print(f\"ğŸ“ Yeni Durum: {obs}\")\n",
    "print(f\"ğŸ’¸ AlÄ±nan Ã–dÃ¼l: {reward}\")\n",
    "print(f\"ğŸš¦ BÃ¶lÃ¼m Bitti mi?: {terminated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dab231a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Yeni Durum: [0]\n",
      "ğŸ’¸ AlÄ±nan Ã–dÃ¼l: [-1.]\n",
      "ğŸš¦ BÃ¶lÃ¼m Bitti mi?: [False]\n"
     ]
    }
   ],
   "source": [
    "# Tahmin edilen eylemi uygula ve 4 deÄŸer geri al\n",
    "obs, reward, done, info = env.step(action)\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±rarak kontrol edelim\n",
    "print(f\"ğŸ“ Yeni Durum: {obs}\")\n",
    "print(f\"ğŸ’¸ AlÄ±nan Ã–dÃ¼l: {reward}\")\n",
    "print(f\"ğŸš¦ BÃ¶lÃ¼m Bitti mi?: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ğŸ–¼ï¸ OrtamÄ± GÃ¶rselleÅŸtirin\n",
    "\n",
    "Eylem gerÃ§ekleÅŸtirildikten sonra ortamÄ±n mevcut durumunu gÃ¶rselleÅŸtirmek iÃ§in `.render()` kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# AjanÄ±n hamle sonrasÄ± konumunu pencerede gÃ¶r\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  ğŸ§¹ OrtamÄ± KapatÄ±n\n",
    "\n",
    "OrtamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatmak ve sistem kaynaklarÄ±nÄ± serbest bÄ±rakmak iÃ§in `.close()` kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# Sistem kaynaklarÄ±nÄ± serbest bÄ±rakmak iÃ§in ortamÄ± kapat\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### BÃ¶lÃ¼m 7: Tam EtkileÅŸim DÃ¶ngÃ¼sÃ¼nÃ¼ UygulayÄ±n\n",
    "\n",
    "Bu son alÄ±ÅŸtÄ±rmada, Ã¶nceden eÄŸitilmiÅŸ modelinizin bÃ¶lÃ¼m bitene kadar ortamla adÄ±m adÄ±m etkileÅŸim kurduÄŸu tam bir dÃ¶ngÃ¼ oluÅŸturacaksÄ±nÄ±z.\n",
    "\n",
    "- Eylemleri tekrarlamak iÃ§in bir `while` dÃ¶ngÃ¼sÃ¼ kullanÄ±n.\n",
    "- Eylemleri seÃ§mek iÃ§in `.predict()` ve bunlarÄ± uygulamak iÃ§in `.step()` Ã§aÄŸÄ±rÄ±n.\n",
    "- Bir sonraki predict iterasyonuna yeni durumu beslediÄŸinizden emin olun.\n",
    "- Ne zaman dÃ¶ngÃ¼den Ã§Ä±kacaÄŸÄ±nÄ±zÄ± bilmek iÃ§in `.step()`'den gelen `done` bayraÄŸÄ±nÄ± kullanÄ±n.\n",
    "- Her adÄ±mdan sonra ajanÄ± aksiyon halinde gÃ¶rselleÅŸtirmek iÃ§in `.render()` Ã§aÄŸÄ±rmayÄ± unutmayÄ±n.\n",
    "\n",
    "Yine, ajanÄ±nÄ±z garip hareketler seÃ§erse veya sonsuz dÃ¶ngÃ¼ye girerse, paniklemeyÄ±n: muhtemelen yeterince uzun eÄŸitim gÃ¶rmemiÅŸtir. AjanÄ±nÄ±z sonsuz dÃ¶ngÃ¼ye girdiyse, hÃ¼crenin yÃ¼rÃ¼tÃ¼lmesini durdurun.\n",
    "\n",
    "ğŸ‘‰ Daha uzun eÄŸitmeyi denemeden Ã¶nce meydan okumanÄ±n sonuna kadar gidin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajan yÃ¼rÃ¼yÃ¼ÅŸe baÅŸlÄ±yor...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Her adÄ±mdan sonra ajanÄ± gÃ¶rselleÅŸtirin\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 5. Ä°ÅŸlem bittiÄŸinde ortamÄ± kapatÄ±n\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAjan hedefe ulaÅŸtÄ± veya gÃ¶rev bitti! ğŸ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:103\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:250\u001b[0m, in \u001b[0;36mVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# mode == self.render_mode == \"human\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# In that case, we try to call `self.env.render()` but it might\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# crash for subprocesses\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# call the render method of the environments\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:129\u001b[0m, in \u001b[0;36mDummyVecEnv.env_method\u001b[0;34m(self, method_name, indices, *method_args, **method_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call instance methods of vectorized environments.\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m target_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_envs(indices)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43menv_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_wrapper_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m target_envs]\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:228\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:315\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    313\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    314\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# rgb_array\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    318\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    319\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. Ã–nceden eÄŸitilmiÅŸ modeli yÃ¼kleyin (Listende gÃ¶rdÃ¼ÄŸÃ¼mÃ¼z ismi kullanÄ±yoruz)\n",
    "from stable_baselines3 import DQN\n",
    "model = DQN.load(\"dqn_cliffwalking_v1\")\n",
    "\n",
    "# 2. OrtamÄ± hazÄ±rlayÄ±n ve SB3 ile uyumlu hale getirin\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# 3. Yeni bir bÃ¶lÃ¼me baÅŸlayÄ±n\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "# 4. EtkileÅŸim dÃ¶ngÃ¼sÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±n\n",
    "print(\"Ajan yÃ¼rÃ¼yÃ¼ÅŸe baÅŸlÄ±yor...\")\n",
    "while not done:\n",
    "    # Modelin mevcut duruma gÃ¶re en iyi hamleyi tahmin etmesini isteyin\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Bu hamleyi uygulayÄ±n (Unutma, SB3 sarmalayÄ±cÄ±sÄ± 4 deÄŸer dÃ¶ndÃ¼rÃ¼r)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # Her adÄ±mdan sonra ajanÄ± gÃ¶rselleÅŸtirin\n",
    "    env.render()\n",
    "\n",
    "# 5. Ä°ÅŸlem bittiÄŸinde ortamÄ± kapatÄ±n\n",
    "print(\"Ajan hedefe ulaÅŸtÄ± veya gÃ¶rev bitti! ğŸ\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec25a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uzman ajan parkura Ã§Ä±kÄ±yor...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# CanlÄ± izle\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ TEBRÄ°KLER! Ajan uÃ§uruma dÃ¼ÅŸmeden hedefe ulaÅŸtÄ±.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:103\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:250\u001b[0m, in \u001b[0;36mVecEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# mode == self.render_mode == \"human\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# In that case, we try to call `self.env.render()` but it might\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# crash for subprocesses\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# call the render method of the environments\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:129\u001b[0m, in \u001b[0;36mDummyVecEnv.env_method\u001b[0;34m(self, method_name, indices, *method_args, **method_kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call instance methods of vectorized environments.\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m target_envs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_envs(indices)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43menv_i\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_wrapper_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env_i \u001b[38;5;129;01min\u001b[39;00m target_envs]\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:228\u001b[0m, in \u001b[0;36mCliffWalkingEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/reinforcement-env/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:315\u001b[0m, in \u001b[0;36mCliffWalkingEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    313\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    314\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# rgb_array\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    318\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    319\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 1. Uzman modeli yÃ¼kle\n",
    "model = DQN.load(\"dqn_cliffwalking_fast\")\n",
    "\n",
    "# 2. OrtamÄ± GÃ–RSEL modda tekrar oluÅŸtur\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# 3. Testi baÅŸlat\n",
    "obs = env.reset()\n",
    "done = [False] # DummyVecEnv iÃ§in liste formatÄ±nda baÅŸlatÄ±yoruz\n",
    "\n",
    "print(\"Uzman ajan parkura Ã§Ä±kÄ±yor...\")\n",
    "\n",
    "while not done[0]:\n",
    "    # Modelin en iyi hamleyi yapmasÄ±nÄ± saÄŸla\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Hamleyi uygula\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # CanlÄ± izle\n",
    "    env.render()\n",
    "\n",
    "print(\"ğŸ TEBRÄ°KLER! Ajan uÃ§uruma dÃ¼ÅŸmeden hedefe ulaÅŸtÄ±.\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba4c28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "EÄŸitim baÅŸlÄ±yor, lÃ¼tfen bekleyin...\n",
      "Logging to ./dqn_cliff_tensorboard/DQN_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0364331c99774c36b2333af6397f6d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tebrikler! Uzman model 'dqn_cliffwalking_fast' adÄ±yla kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 1. OrtamÄ± gÃ¶rselleÅŸtirme olmadan, en hÄ±zlÄ± modda oluÅŸturun\n",
    "env = gym.make('CliffWalking-v0', render_mode=None)\n",
    "\n",
    "# 2. Stable Baselines3 ile uyumlu olmasÄ± iÃ§in ortamÄ± sarmalayÄ±n\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# 3. DQN modelini (beyni) bu ortama baÄŸlÄ± olacak ÅŸekilde oluÅŸturun\n",
    "model = DQN(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    verbose=1, \n",
    "    tensorboard_log=\"./dqn_cliff_tensorboard\"\n",
    ")\n",
    "\n",
    "# 4. 100.000 adÄ±mlÄ±k bÃ¼yÃ¼k eÄŸitimi baÅŸlatÄ±n\n",
    "total_timesteps = 100_000\n",
    "print(\"EÄŸitim baÅŸlÄ±yor, lÃ¼tfen bekleyin...\")\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "\n",
    "# 5. EÄŸitilen bu uzman beyni hemen kaydedin\n",
    "model.save(\"dqn_cliffwalking_fast\")\n",
    "print(\"Tebrikler! Uzman model 'dqn_cliffwalking_fast' adÄ±yla kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” AjanÄ±nÄ±zÄ±n DavranÄ±ÅŸÄ±ndan Memnun DeÄŸil misiniz?\n",
    "\n",
    "EndiÅŸelenmeyin â€” seÃ§enekleriniz var:\n",
    "\n",
    "- ğŸ‹ï¸â€â™‚ï¸ **Daha uzun eÄŸitin** â†’ PerformansÄ± artÄ±rmak iÃ§in bÃ¶lÃ¼m sayÄ±sÄ±nÄ± artÄ±rmayÄ± deneyin.\n",
    "- ğŸ“¦ **Veya bizim Ã¶nceden eÄŸitilmiÅŸ modelimizi yÃ¼kleyin** â†’  \n",
    "  **500.000 bÃ¶lÃ¼m** iÃ§in bir tane eÄŸittik â€” indirmek iÃ§in aÅŸaÄŸÄ±daki hÃ¼crenin yorumunu kaldÄ±rÄ±n ve Ã§alÄ±ÅŸtÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  139k  100  139k    0     0  94924      0  0:00:01  0:00:01 --:--:-- 94946\n"
     ]
    }
   ],
   "source": [
    "!curl https://d37p7d5kaxknzw.cloudfront.net/projects/best_dqn_cliffwalking.zip > best_dqn_cliffwalking.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "126e3cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Åampiyon ajan 500.000 bÃ¶lÃ¼mlÃ¼k tecrÃ¼besiyle parkura Ã§Ä±kÄ±yor...\n",
      "MÃ¼kemmel! Ajan uÃ§uruma hiÃ§ yaklaÅŸmadan hedefe ulaÅŸtÄ±. ğŸ\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 1. Ä°ndirilen \"En Ä°yi\" (Best) modeli diskten yÃ¼kleyin\n",
    "# SB3 .load() kullanÄ±rken dosya uzantÄ±sÄ±nÄ± (.zip) yazmanÄ±za gerek yoktur.\n",
    "model = DQN.load(\"best_dqn_cliffwalking\")\n",
    "\n",
    "# 2. OrtamÄ± gÃ¶rselleÅŸtirme (human mode) ile tekrar oluÅŸturun\n",
    "env = gym.make('CliffWalking-v0', render_mode='human')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# 3. OrtamÄ± sÄ±fÄ±rlayÄ±n ve baÅŸlangÄ±Ã§ verisini alÄ±n\n",
    "obs = env.reset()\n",
    "done = [False]\n",
    "\n",
    "print(\"Åampiyon ajan 500.000 bÃ¶lÃ¼mlÃ¼k tecrÃ¼besiyle parkura Ã§Ä±kÄ±yor...\")\n",
    "\n",
    "# 4. AkÄ±llÄ± etkileÅŸim dÃ¶ngÃ¼sÃ¼\n",
    "while not done[0]:\n",
    "    # Modelin en gÃ¼venli hamleyi yapmasÄ±nÄ± saÄŸlayÄ±n\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Hamleyi gerÃ§ekleÅŸtirin\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    # CanlÄ± olarak izleyin\n",
    "    env.render()\n",
    "\n",
    "print(\"MÃ¼kemmel! Ajan uÃ§uruma hiÃ§ yaklaÅŸmadan hedefe ulaÅŸtÄ±. ğŸ\")\n",
    "\n",
    "# 5. Ä°ÅŸlem bittiÄŸinde pencereyi kapatÄ±n\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â„ï¸ Son not: `is_slippery=True` Anlamak\n",
    "\n",
    "DokÃ¼mantasyonu okurken `is_slippery` parametresinin ne yaptÄ±ÄŸÄ±nÄ± merak etmiÅŸ olabilirsiniz.\n",
    "\n",
    "`CliffWalking`'de `is_slippery=True` ayarlamak, ajanÄ±n eylemlerine rastgelelik ekler â€” ajanÄ±n kayÄ±p istenmeyen bir yÃ¶ne hareket etme ÅŸansÄ±nÄ± tanÄ±tÄ±r. `is_slippery=True` ile eÄŸitim yapmak, belirsizlikle baÅŸa Ã§Ä±kabilen ajanlar oluÅŸturur â€” gerÃ§ek dÃ¼nya uygulamalarÄ± iÃ§in kritik bir beceri.\n",
    "\n",
    "#### ğŸ¯ Neden stokastisite ekleyelim?\n",
    "\n",
    "- **ğŸŒ GerÃ§ekÃ§ilik** â†’ GerÃ§ek dÃ¼nya ortamlarÄ±nda bulunan belirsizliÄŸi simÃ¼le eder.  \n",
    "- **ğŸ›¡ï¸ SaÄŸlamlÄ±k** â†’ AjanlarÄ±n daha gÃ¼venilir, uyum saÄŸlayabilen stratejiler Ã¶ÄŸrenmesine yardÄ±mcÄ± olur.  \n",
    "- **ğŸ”¥ Meydan okuma** â†’ GÃ¶revi daha zor ve Ã§Ã¶zÃ¼lmesi daha ilginÃ§ hale getirir.\n",
    "\n",
    "#### ğŸ¤” Peki neden burada `True` yapmadÄ±k?\n",
    "\n",
    "DQN algoritmasÄ± basit kullanÄ±m durumlarÄ± iÃ§in tasarlanmÄ±ÅŸtÄ±r. `slippery=True`'nun `CliffWalking` ortamÄ±na eklediÄŸi yÃ¼ksek stokastisite ile mÃ¼cadele eder. Åunu dÃ¼ÅŸÃ¼nÃ¼n: bu basit ortamda, amaÃ§lanan yÃ¶nÃ¼ takip etmemek hemen tamamen farklÄ± bir yÃ¶ne gitmek anlamÄ±na gelir: 90 derece, hatta 180 derece! Basit bir algoritmanÄ±n Ã¶ÄŸrenmesi Ã§ok zor olurdu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeyi tamamladÄ±ÄŸÄ±nÄ±z iÃ§in tebrikler ğŸ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
